{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk \n",
    "import langid\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dic(dic,df,sheet):\n",
    "    str1='' \n",
    "#FIND ENGLISH AND JOIN INTO DICTIONATY for each sheet\n",
    "    print(f\"processsing sheet{sheet}.\")\n",
    "    for i in df.text.values: \n",
    "        lang = langid.classify(str(i))\n",
    "        if lang and 'en'in lang[0]:\n",
    "            str1 = str1 +'\\n'+ str(i)           \n",
    "            dic[sheet]= str1\n",
    "    return dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processsing sheet2020-03-22.\n",
      "processsing sheet2020-03-23.\n",
      "processsing sheet2020-03-24.\n",
      "processsing sheet2020-03-25.\n",
      "processsing sheet2020-03-26.\n",
      "processsing sheet2020-03-27.\n",
      "processsing sheet2020-03-28.\n",
      "processsing sheet2020-03-29.\n",
      "processsing sheet2020-03-30.\n",
      "processsing sheet2020-03-31.\n",
      "processsing sheet2020-04-01.\n",
      "processsing sheet2020-04-02.\n",
      "processsing sheet2020-04-03.\n",
      "processsing sheet2020-04-04.\n",
      "processsing sheet2020-04-05.\n",
      "processsing sheet2020-04-06.\n",
      "processsing sheet2020-04-07.\n",
      "processsing sheet2020-04-08.\n",
      "processsing sheet2020-04-09.\n",
      "processsing sheet2020-04-10.\n",
      "processsing sheet2020-04-11.\n",
      "processsing sheet2020-04-12.\n",
      "processsing sheet2020-04-13.\n",
      "processsing sheet2020-04-14.\n",
      "processsing sheet2020-04-15.\n",
      "processsing sheet2020-04-16.\n",
      "processsing sheet2020-04-17.\n",
      "processsing sheet2020-04-18.\n",
      "processsing sheet2020-04-19.\n",
      "processsing sheet2020-04-20.\n",
      "processsing sheet2020-04-21.\n",
      "processsing sheet2020-04-22.\n",
      "processsing sheet2020-04-23.\n",
      "processsing sheet2020-04-24.\n",
      "processsing sheet2020-04-25.\n",
      "processsing sheet2020-04-26.\n",
      "processsing sheet2020-04-27.\n",
      "processsing sheet2020-04-28.\n",
      "processsing sheet2020-04-29.\n",
      "processsing sheet2020-04-30.\n",
      "processsing sheet2020-05-01.\n",
      "processsing sheet2020-05-02.\n",
      "processsing sheet2020-05-03.\n",
      "processsing sheet2020-05-04.\n",
      "processsing sheet2020-05-05.\n",
      "processsing sheet2020-05-06.\n",
      "processsing sheet2020-05-07.\n",
      "processsing sheet2020-05-08.\n",
      "processsing sheet2020-05-09.\n",
      "processsing sheet2020-05-10.\n",
      "processsing sheet2020-05-11.\n",
      "processsing sheet2020-05-12.\n",
      "processsing sheet2020-05-13.\n",
      "processsing sheet2020-05-14.\n",
      "processsing sheet2020-05-15.\n",
      "processsing sheet2020-05-16.\n",
      "processsing sheet2020-05-17.\n",
      "processsing sheet2020-05-18.\n",
      "processsing sheet2020-05-19.\n",
      "processsing sheet2020-05-20.\n",
      "processsing sheet2020-05-21.\n",
      "processsing sheet2020-05-22.\n",
      "processsing sheet2020-05-23.\n",
      "processsing sheet2020-05-24.\n",
      "processsing sheet2020-05-25.\n",
      "processsing sheet2020-05-26.\n",
      "processsing sheet2020-05-27.\n",
      "processsing sheet2020-05-28.\n",
      "processsing sheet2020-05-29.\n",
      "processsing sheet2020-05-30.\n",
      "processsing sheet2020-05-31.\n",
      "processsing sheet2020-06-01.\n",
      "processsing sheet2020-06-02.\n",
      "processsing sheet2020-06-03.\n",
      "processsing sheet2020-06-04.\n",
      "processsing sheet2020-06-05.\n",
      "processsing sheet2020-06-06.\n",
      "processsing sheet2020-06-07.\n",
      "processsing sheet2020-06-08.\n",
      "processsing sheet2020-06-09.\n",
      "processsing sheet2020-06-10.\n"
     ]
    }
   ],
   "source": [
    "excel = pd.ExcelFile('28130006.xlsx') # opening excel fiile\n",
    "d={} # declaring an empty dict\n",
    "for sheet in excel.sheet_names: # iterate over each sheets\n",
    "    df = excel.parse(sheet)\n",
    "    df = df.dropna(axis=1,how='all')\n",
    "    df = df.dropna(axis=0,how='all')\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    if df.iloc[0][0]== \"text\":\n",
    "      \n",
    "        new_header = df.iloc[0] #grab the first row for the header\n",
    "        df = df[1:] #take the data less the header row\n",
    "        df.columns = new_header #set the header row as the df header\n",
    "        df.head()\n",
    "        d=add_to_dic(d,df,sheet)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        d=add_to_dic(d,df,sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer the Data Dictionary\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#FIND WORDS FROM SENTENCES- SEPERATE WORDS AND MAKE LOWERCASE\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "\n",
    "def tokenizedata(sheet): #from Tut05\n",
    "    \"\"\"\n",
    "        the tokenization function is used to tokenize raw data in Dictionary\n",
    "    \"\"\"\n",
    "    raw_data = d[sheet].lower() \n",
    "    unigram_tokens = tokenizer.tokenize(raw_data)\n",
    "    return (sheet, unigram_tokens)\n",
    "\n",
    "##First Dictionary with all the tokens for each Day\n",
    "data_tokenized = dict(tokenizedata(sheet) for sheet in d.keys())\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_tokenized.keys()) #to check if all the sheets have been captured i.e 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Dict for most Frequent 100 bigrams for each day\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import *\n",
    "collect_bi={}\n",
    "for keys,values in data_tokenized.items():\n",
    "    bi=[]\n",
    "    for i in ngrams(values,2):\n",
    "        bi.append(i)\n",
    "    fdbigram = FreqDist(bi)\n",
    "    collect_bi[keys]=fdbigram.most_common(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the Dict of 100 Bigrams to file\n",
    "save_file = open(\"arushi_100bigrams.txt\", 'w')\n",
    "for k,v, in collect_bi.items():\n",
    "    save_file.write(\"%s:%s\\n\"%(k,v))\n",
    "  \n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  199616 \n",
      "Total number of tokens:  2306778 \n",
      "Lexical diversity:  11.55607766912472\n"
     ]
    }
   ],
   "source": [
    "#from Tut05----- start cleaning up and proceed with Vocab\n",
    "from __future__ import division\n",
    "from itertools import chain\n",
    "import itertools\n",
    "all_words = list(chain.from_iterable(data_tokenized.values()))\n",
    "#Make a Voc\n",
    "all_vocab = set(all_words)\n",
    "lexical_diversity = len(all_words)/len(all_vocab)\n",
    "print (\"Vocabulary size: \",len(all_vocab),\"\\nTotal number of tokens: \", len(all_words), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 91617),\n",
       " ('co', 86730),\n",
       " ('https', 86522),\n",
       " ('the', 70496),\n",
       " ('to', 51822),\n",
       " ('covid', 41860),\n",
       " ('of', 36085),\n",
       " ('coronavirus', 31317),\n",
       " ('and', 30969),\n",
       " ('in', 29697)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is to check the top most words for the whole Excel file\n",
    "fd_1 = FreqDist(all_words)\n",
    "fd_1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('around', 81),\n",
       " ('important', 81),\n",
       " ('hospital', 81),\n",
       " ('together', 81),\n",
       " ('best', 81),\n",
       " ('fight', 81),\n",
       " (\"let's\", 81),\n",
       " ('long', 81),\n",
       " ('action', 81),\n",
       " ('like', 81)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for Document Frequency to find out Rare Tokens and \n",
    "words_2 = list(chain.from_iterable([set(value) for value in data_tokenized.values()]))\n",
    "fd_2 = FreqDist(words_2)\n",
    "fd_2.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "data_tokenized_1 = {}\n",
    "stopwordsSet = set(stopwords)\n",
    "for sheet in data_tokenized.keys():\n",
    "    data_tokenized_1[sheet] = [w for w in data_tokenized[sheet] if w not in stopwordsSet]\n",
    "    \n",
    "#data_tokenized = dict(tokenizedata(sheet) for sheet in d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_tokenized_1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['six',\n",
       " 'is',\n",
       " 'until',\n",
       " 'liked',\n",
       " \"haven't\",\n",
       " 'me',\n",
       " 'again',\n",
       " 'around',\n",
       " \"weren't\",\n",
       " 'might',\n",
       " 'done',\n",
       " 'indeed',\n",
       " 'hi',\n",
       " 'doing',\n",
       " 'lest',\n",
       " 'g',\n",
       " 'us',\n",
       " 'certain',\n",
       " 'someone',\n",
       " 'uses',\n",
       " 'being',\n",
       " 'if',\n",
       " 'none',\n",
       " 'ltd',\n",
       " 'different',\n",
       " 'knows',\n",
       " 'merely',\n",
       " 'should',\n",
       " 'just',\n",
       " 'would',\n",
       " 'seriously',\n",
       " 'it',\n",
       " \"aren't\",\n",
       " \"you've\",\n",
       " 'herself',\n",
       " 'several',\n",
       " 'often',\n",
       " 'inc',\n",
       " 'former',\n",
       " 'greetings',\n",
       " 'go',\n",
       " 'concerning',\n",
       " 'become',\n",
       " 'since',\n",
       " 'less',\n",
       " 'most',\n",
       " 'your',\n",
       " 'up',\n",
       " 'downwards',\n",
       " 'others',\n",
       " 'thank',\n",
       " 'inward',\n",
       " 'f',\n",
       " 'come',\n",
       " 'yes',\n",
       " 'saying',\n",
       " 'yet',\n",
       " 'overall',\n",
       " 'que',\n",
       " 'under',\n",
       " 'actually',\n",
       " 'see',\n",
       " 'etc',\n",
       " 'ask',\n",
       " 'somehow',\n",
       " 'seeming',\n",
       " 'aside',\n",
       " 'vs',\n",
       " 'hence',\n",
       " 'below',\n",
       " 'while',\n",
       " 'when',\n",
       " 'seeing',\n",
       " \"we've\",\n",
       " 'otherwise',\n",
       " 'always',\n",
       " 'ignored',\n",
       " 'himself',\n",
       " 'thanx',\n",
       " 'nobody',\n",
       " 'together',\n",
       " 'among',\n",
       " 'latter',\n",
       " 'which',\n",
       " 'against',\n",
       " 'particular',\n",
       " 'thoroughly',\n",
       " 'toward',\n",
       " 'indicated',\n",
       " 'seem',\n",
       " 'sup',\n",
       " \"wasn't\",\n",
       " 'associated',\n",
       " 'be',\n",
       " 'respectively',\n",
       " 'wherein',\n",
       " 'specified',\n",
       " 'nd',\n",
       " 'seen',\n",
       " 'mainly',\n",
       " 'every',\n",
       " 'n',\n",
       " 'neither',\n",
       " \"that's\",\n",
       " 'across',\n",
       " 'secondly',\n",
       " 'take',\n",
       " \"can't\",\n",
       " 'thats',\n",
       " 'indicates',\n",
       " 'hereby',\n",
       " 'taken',\n",
       " 'via',\n",
       " 'are',\n",
       " 'tried',\n",
       " 'wonder',\n",
       " 'became',\n",
       " 'please',\n",
       " 'sub',\n",
       " 'kept',\n",
       " 'placed',\n",
       " 'likely',\n",
       " 'gets',\n",
       " 'viz',\n",
       " 'ours',\n",
       " 'try',\n",
       " 'j',\n",
       " 'value',\n",
       " 'really',\n",
       " 'there',\n",
       " 'everywhere',\n",
       " 'causes',\n",
       " 'everybody',\n",
       " 'l',\n",
       " 'has',\n",
       " \"doesn't\",\n",
       " 'think',\n",
       " 'few',\n",
       " 'too',\n",
       " 'anyhow',\n",
       " 'somebody',\n",
       " 'not',\n",
       " \"here's\",\n",
       " 'thru',\n",
       " 'best',\n",
       " 'onto',\n",
       " 'presumably',\n",
       " 'currently',\n",
       " 'asking',\n",
       " \"i'm\",\n",
       " 'containing',\n",
       " 'c',\n",
       " 'unlikely',\n",
       " 'th',\n",
       " 'outside',\n",
       " 'u',\n",
       " 'happens',\n",
       " 'either',\n",
       " 'whatever',\n",
       " 'namely',\n",
       " 'whereas',\n",
       " 'plus',\n",
       " 'z',\n",
       " 'indicate',\n",
       " 'anyway',\n",
       " 'needs',\n",
       " 'zero',\n",
       " 'that',\n",
       " 'obviously',\n",
       " 'thanks',\n",
       " 'sensible',\n",
       " 'our',\n",
       " 'forth',\n",
       " 'eight',\n",
       " 'soon',\n",
       " 'his',\n",
       " \"wouldn't\",\n",
       " 'she',\n",
       " 'by',\n",
       " 'two',\n",
       " \"hadn't\",\n",
       " 'along',\n",
       " 'appreciate',\n",
       " 'qv',\n",
       " 'why',\n",
       " 'second',\n",
       " 'relatively',\n",
       " 'ok',\n",
       " 'thereby',\n",
       " \"let's\",\n",
       " 'perhaps',\n",
       " 'w',\n",
       " 'came',\n",
       " 'm',\n",
       " 'truly',\n",
       " 'beforehand',\n",
       " 'using',\n",
       " 'getting',\n",
       " 'could',\n",
       " 'various',\n",
       " 'i',\n",
       " 'corresponding',\n",
       " \"didn't\",\n",
       " 'does',\n",
       " 'so',\n",
       " 'also',\n",
       " 'ie',\n",
       " 'accordingly',\n",
       " 'okay',\n",
       " 'like',\n",
       " 'between',\n",
       " 'on',\n",
       " 'were',\n",
       " 'gotten',\n",
       " \"isn't\",\n",
       " 'anywhere',\n",
       " 'according',\n",
       " 'therefore',\n",
       " 'sometimes',\n",
       " 'may',\n",
       " 'enough',\n",
       " 'allow',\n",
       " 'gives',\n",
       " 'especially',\n",
       " 'anyone',\n",
       " \"i've\",\n",
       " 'e',\n",
       " 'took',\n",
       " 'seems',\n",
       " 'first',\n",
       " 't',\n",
       " \"you're\",\n",
       " 'per',\n",
       " 'who',\n",
       " 'thereafter',\n",
       " 'for',\n",
       " 'entirely',\n",
       " \"it's\",\n",
       " 'need',\n",
       " 'definitely',\n",
       " 'each',\n",
       " \"we'd\",\n",
       " 'know',\n",
       " \"he's\",\n",
       " 'with',\n",
       " 'comes',\n",
       " 'theirs',\n",
       " 'said',\n",
       " 'last',\n",
       " 'those',\n",
       " 'another',\n",
       " 'goes',\n",
       " 'apart',\n",
       " 'meanwhile',\n",
       " 'anyways',\n",
       " 'having',\n",
       " 'given',\n",
       " 'here',\n",
       " 'tell',\n",
       " 'looks',\n",
       " 'x',\n",
       " 'later',\n",
       " 'am',\n",
       " 'want',\n",
       " 'everything',\n",
       " 'its',\n",
       " 'afterwards',\n",
       " 'my',\n",
       " 'very',\n",
       " 'let',\n",
       " 'cannot',\n",
       " 'thus',\n",
       " 'y',\n",
       " 'despite',\n",
       " 'whose',\n",
       " 'b',\n",
       " 'serious',\n",
       " 'get',\n",
       " 'anything',\n",
       " 'quite',\n",
       " 'went',\n",
       " 'as',\n",
       " 'better',\n",
       " 'although',\n",
       " 'followed',\n",
       " 'follows',\n",
       " 'whole',\n",
       " 'towards',\n",
       " 'right',\n",
       " \"they're\",\n",
       " 'shall',\n",
       " 'almost',\n",
       " 'an',\n",
       " 'r',\n",
       " 'least',\n",
       " 'out',\n",
       " 'co',\n",
       " 'down',\n",
       " 'yourselves',\n",
       " 'oh',\n",
       " 'above',\n",
       " 'hello',\n",
       " 'hardly',\n",
       " 'into',\n",
       " 'v',\n",
       " 'used',\n",
       " 'course',\n",
       " 'brief',\n",
       " 'yours',\n",
       " 'five',\n",
       " 'whereby',\n",
       " 'necessary',\n",
       " 'their',\n",
       " 'formerly',\n",
       " 'reasonably',\n",
       " 'new',\n",
       " 'normally',\n",
       " 'to',\n",
       " 'wants',\n",
       " 'available',\n",
       " 'whenever',\n",
       " 'must',\n",
       " 'selves',\n",
       " 'regards',\n",
       " 'after',\n",
       " 'sorry',\n",
       " 'help',\n",
       " \"couldn't\",\n",
       " \"you'll\",\n",
       " 'unto',\n",
       " 'thorough',\n",
       " \"you'd\",\n",
       " 'un',\n",
       " 'certainly',\n",
       " \"t's\",\n",
       " \"won't\",\n",
       " 'd',\n",
       " 'p',\n",
       " 'tends',\n",
       " 'inasmuch',\n",
       " 'particularly',\n",
       " 'wherever',\n",
       " \"where's\",\n",
       " \"it'll\",\n",
       " 'looking',\n",
       " 'the',\n",
       " 'hopefully',\n",
       " 'this',\n",
       " 'but',\n",
       " 'myself',\n",
       " 'h',\n",
       " 'or',\n",
       " 'whom',\n",
       " 'and',\n",
       " 'without',\n",
       " \"we're\",\n",
       " 'old',\n",
       " 'lately',\n",
       " \"they'll\",\n",
       " 'maybe',\n",
       " 'elsewhere',\n",
       " \"who's\",\n",
       " 'upon',\n",
       " 'many',\n",
       " 'sent',\n",
       " 'amongst',\n",
       " 'though',\n",
       " 'itself',\n",
       " 'appear',\n",
       " 'regardless',\n",
       " 'eg',\n",
       " 'contain',\n",
       " 'et',\n",
       " 'same',\n",
       " 'welcome',\n",
       " 'you',\n",
       " 'going',\n",
       " 'specify',\n",
       " 'him',\n",
       " 'they',\n",
       " 'beyond',\n",
       " 'com',\n",
       " 'beside',\n",
       " 'a',\n",
       " 'then',\n",
       " 'use',\n",
       " 'been',\n",
       " 'where',\n",
       " 's',\n",
       " 'little',\n",
       " 'allows',\n",
       " 'will',\n",
       " 'k',\n",
       " 'was',\n",
       " 'themselves',\n",
       " 'however',\n",
       " 'sure',\n",
       " 'even',\n",
       " 'mostly',\n",
       " 'during',\n",
       " 'theres',\n",
       " 'before',\n",
       " 'contains',\n",
       " 'how',\n",
       " 'else',\n",
       " 'away',\n",
       " 'provides',\n",
       " 'throughout',\n",
       " 'noone',\n",
       " 'got',\n",
       " 'wish',\n",
       " 'far',\n",
       " 'ever',\n",
       " 'nowhere',\n",
       " 'more',\n",
       " 'nothing',\n",
       " 'fifth',\n",
       " 'do',\n",
       " 'alone',\n",
       " 'never',\n",
       " 'because',\n",
       " 'keeps',\n",
       " 'whether',\n",
       " 'have',\n",
       " 'following',\n",
       " 'gone',\n",
       " 'insofar',\n",
       " 'hers',\n",
       " 'now',\n",
       " 'way',\n",
       " 'nearly',\n",
       " 'four',\n",
       " 'much',\n",
       " 'becomes',\n",
       " \"i'd\",\n",
       " 'all',\n",
       " 'only',\n",
       " 'already',\n",
       " 'ourselves',\n",
       " 'at',\n",
       " 'yourself',\n",
       " 'within',\n",
       " 'both',\n",
       " 'except',\n",
       " \"don't\",\n",
       " 'whoever',\n",
       " 'becoming',\n",
       " 'than',\n",
       " 'seemed',\n",
       " 'once',\n",
       " \"what's\",\n",
       " 'self',\n",
       " 'ones',\n",
       " 'her',\n",
       " 'we',\n",
       " 'exactly',\n",
       " 'ought',\n",
       " \"hasn't\",\n",
       " 'rd',\n",
       " \"they'd\",\n",
       " 'described',\n",
       " 'somewhat',\n",
       " 'o',\n",
       " 'cause',\n",
       " 'had',\n",
       " 'able',\n",
       " 'what',\n",
       " 'consequently',\n",
       " 'believe',\n",
       " \"they've\",\n",
       " 'inner',\n",
       " 'useful',\n",
       " 'cant',\n",
       " 'in',\n",
       " 'did',\n",
       " 'specifying',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'says',\n",
       " 'near',\n",
       " 'moreover',\n",
       " 'mean',\n",
       " 'three',\n",
       " 'nine',\n",
       " 'awfully',\n",
       " 'keep',\n",
       " 'consider',\n",
       " 'furthermore',\n",
       " 'third',\n",
       " 'immediate',\n",
       " 'besides',\n",
       " 'novel',\n",
       " 'behind',\n",
       " 'say',\n",
       " 'example',\n",
       " 'edu',\n",
       " 'about',\n",
       " 'nor',\n",
       " 'regarding',\n",
       " 'he',\n",
       " 'these',\n",
       " 'over',\n",
       " 'tries',\n",
       " \"there's\",\n",
       " 'non',\n",
       " 'still',\n",
       " 'next',\n",
       " \"it'd\",\n",
       " 'them',\n",
       " 'changes',\n",
       " 'rather',\n",
       " 'appropriate',\n",
       " 'something',\n",
       " 'any',\n",
       " \"c'mon\",\n",
       " 'hereafter',\n",
       " 'other',\n",
       " 'willing',\n",
       " 'further',\n",
       " 'seven',\n",
       " 're',\n",
       " 'clearly',\n",
       " 'can',\n",
       " 'saw',\n",
       " 'known',\n",
       " 'ex',\n",
       " 'twice',\n",
       " 'one',\n",
       " 'no',\n",
       " 'unfortunately',\n",
       " 'from',\n",
       " \"ain't\",\n",
       " \"we'll\",\n",
       " 'anybody',\n",
       " \"shouldn't\",\n",
       " 'unless',\n",
       " 'trying',\n",
       " 'herein',\n",
       " 'well',\n",
       " 'name',\n",
       " 'own',\n",
       " 'usually',\n",
       " 'possible',\n",
       " 'somewhere',\n",
       " 'considering',\n",
       " 'probably',\n",
       " 'look',\n",
       " 'off',\n",
       " 'some',\n",
       " 'nevertheless',\n",
       " 'through',\n",
       " 'such',\n",
       " 'everyone',\n",
       " 'sometime',\n",
       " \"i'll\",\n",
       " 'q']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Lets create chain of words (bags of words) after StopWords removed Word3\n",
    "words_3 = list(chain.from_iterable(data_tokenized_1.values()))\n",
    "fd_3 = FreqDist(words_3)\n",
    "list(all_vocab - set(fd_3.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https', 86522),\n",
       " ('covid', 41860),\n",
       " ('coronavirus', 31317),\n",
       " ('people', 6638),\n",
       " ('amp', 6494),\n",
       " ('pandemic', 5505),\n",
       " ('cases', 5110),\n",
       " ('news', 3869),\n",
       " ('lockdown', 3845),\n",
       " ('trump', 3663)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is to check the top most words_3 which are after stop words\n",
    "fd_1 = FreqDist(words_3)\n",
    "fd_1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('important', 81),\n",
       " ('hospital', 81),\n",
       " ('fight', 81),\n",
       " ('long', 81),\n",
       " ('action', 81),\n",
       " ('covid', 81),\n",
       " ('love', 81),\n",
       " ('morning', 81),\n",
       " ('disease', 81),\n",
       " ('years', 81)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Check the document Freq after stop words are removed\n",
    "words_4 = list(chain.from_iterable([set(value) for value in data_tokenized_1.values()]))\n",
    "fd_2 = FreqDist(words_4)\n",
    "fd_2.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15705"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###we need to remove the Rare tokens (5days) and Context_independent /Dependent (60 days)\n",
    "lessFreqWords = [k for k, v in fd_2.items() if v > 4 and v < 60 ]\n",
    "len(lessFreqWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lessFreqWords=set(lessFreqWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data_tokenized_2 = {}\n",
    "def removeLessFreqWords(sheet):\n",
    "    return (sheet, [w for w in data_tokenized_1[sheet] if w in lessFreqWords])\n",
    "\n",
    "data_tokenized_2 = dict(removeLessFreqWords(sheet) for sheet in data_tokenized_1.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_tokenized_2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('protests', 355),\n",
       " ('sunday', 252),\n",
       " ('cummings', 232),\n",
       " ('tuesday', 188),\n",
       " ('alert', 179),\n",
       " ('zealand', 179),\n",
       " ('migrant', 162),\n",
       " ('riots', 157),\n",
       " ('george', 154),\n",
       " ('matthancock', 152)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###new Chain of Words after removing less freq/Rare\n",
    "words_5 = list(chain.from_iterable(data_tokenized_2.values()))\n",
    "fd_1 = FreqDist(words_5)\n",
    "fd_1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('map', 59),\n",
       " ('lets', 59),\n",
       " ('offers', 59),\n",
       " ('games', 59),\n",
       " ('homeless', 59),\n",
       " ('speaking', 59),\n",
       " ('shares', 59),\n",
       " ('highly', 59),\n",
       " ('scared', 59),\n",
       " ('fault', 59)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_6 = list(chain.from_iterable([set(value) for value in data_tokenized_2.values()]))\n",
    "fd_2 = FreqDist(words_6)\n",
    "fd_2.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  15705 \n",
      "Total number of tokens:  350404 \n",
      "Lexical diversity:  22.311620503024514\n"
     ]
    }
   ],
   "source": [
    "#check after rare tokens removed\n",
    "all_vocab_1 = set(words_5)\n",
    "lexical_diversity = len(words_5)/len(all_vocab_1)\n",
    "print (\"Vocabulary size: \",len(all_vocab_1),\"\\nTotal number of tokens: \", len(words_5), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets remove less than 3 char words\n",
    "data_tokenized_3={}\n",
    "for sheet in data_tokenized_2.keys():\n",
    "    data_tokenized_3[sheet] = [w for w in data_tokenized_2[sheet] if len(w) >3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_tokenized_3.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('protests', 355),\n",
       " ('sunday', 252),\n",
       " ('cummings', 232),\n",
       " ('tuesday', 188),\n",
       " ('alert', 179),\n",
       " ('zealand', 179),\n",
       " ('migrant', 162),\n",
       " ('riots', 157),\n",
       " ('george', 154),\n",
       " ('matthancock', 152)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###new Chain of Words after removing less freq/Rare\n",
    "words_7 = list(chain.from_iterable(data_tokenized_3.values()))\n",
    "fd_1 = FreqDist(words_7)\n",
    "fd_1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lets', 59),\n",
       " ('offers', 59),\n",
       " ('games', 59),\n",
       " ('homeless', 59),\n",
       " ('speaking', 59),\n",
       " ('shares', 59),\n",
       " ('highly', 59),\n",
       " ('scared', 59),\n",
       " ('fault', 59),\n",
       " ('immediately', 59)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_8 = list(chain.from_iterable([set(value) for value in data_tokenized_3.values()]))\n",
    "fd_2 = FreqDist(words_8)\n",
    "fd_2.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  13636 \n",
      "Total number of tokens:  294941 \n",
      "Lexical diversity:  21.629583455558816\n"
     ]
    }
   ],
   "source": [
    "all_vocab_2 = set(words_7)\n",
    "lexical_diversity = len(words_7)/len(all_vocab_2)\n",
    "print (\"Vocabulary size: \",len(all_vocab_2),\"\\nTotal number of tokens: \", len(words_7), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Lets do Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "data_tokenized_4={}\n",
    "for sheet in data_tokenized_3.keys():\n",
    "    data_tokenized_4[sheet] = [stemmer.stem(w) for w in data_tokenized_3[sheet]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_tokenized_4.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('protest', 606),\n",
       " ('oper', 401),\n",
       " ('lift', 353),\n",
       " ('donat', 351),\n",
       " ('delay', 330),\n",
       " ('suggest', 309),\n",
       " ('predict', 307),\n",
       " ('celebr', 297),\n",
       " ('migrant', 293),\n",
       " ('save', 289)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_9 = list(chain.from_iterable(data_tokenized_4.values()))\n",
    "fd_1 = FreqDist(words_9)\n",
    "fd_1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('celebr', 80),\n",
       " ('donat', 80),\n",
       " ('lift', 79),\n",
       " ('oper', 79),\n",
       " ('surviv', 79),\n",
       " ('save', 79),\n",
       " ('walk', 79),\n",
       " ('urg', 78),\n",
       " ('support', 78),\n",
       " ('design', 78)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_10 = list(chain.from_iterable([set(value) for value in data_tokenized_4.values()]))\n",
    "fd_2 = FreqDist(words_10)\n",
    "fd_2.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9799 \n",
      "Total number of tokens:  294941 \n",
      "Lexical diversity:  30.099091744055517\n"
     ]
    }
   ],
   "source": [
    "all_vocab_3 = set(words_9)\n",
    "lexical_diversity = len(words_9)/len(all_vocab_3)\n",
    "print (\"Vocabulary size: \",len(all_vocab_3),\"\\nTotal number of tokens: \", len(words_9), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Create Unigrams here from data_tokenized_4\n",
    "#####\n",
    "collect_uni={}\n",
    "for keys,values in data_tokenized_4.items():\n",
    "    uni=[]\n",
    "    for i in values:\n",
    "        uni.append(i)\n",
    "    unigram = FreqDist(uni)\n",
    "    collect_uni[keys]=unigram.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = open(\"arushi_100unigrams.txt\", 'w')\n",
    "for k,v in collect_uni.items():\n",
    "    save_file.write(\"%s:%s\\n\"%(k,v))\n",
    "  \n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dlaignit', 'socialsel'),\n",
       " ('nutan', 'jyot'),\n",
       " ('socialsel', 'digitalsel'),\n",
       " ('tradingeconom', 'macrocalendar'),\n",
       " ('asitha', 'jayawardena'),\n",
       " ('kmnw', 'igf'),\n",
       " ('macrocalendar', 'economiccalendar'),\n",
       " ('roboinfo', 'robotex'),\n",
       " ('sperm', 'smoothi'),\n",
       " ('artcal', 'artistcal'),\n",
       " ('artistcal', 'artistscal'),\n",
       " ('artistscal', 'artpromot'),\n",
       " ('artpromot', 'coronavirusart'),\n",
       " ('meaningfulgrowth', 'geniouxmg'),\n",
       " ('nagendra', 'bandi'),\n",
       " ('nirmala', 'sitharaman'),\n",
       " ('sierra', 'leon'),\n",
       " ('tejasvi', 'surya'),\n",
       " ('foryoupag', 'foryou'),\n",
       " ('agenparl', 'covidagenparl'),\n",
       " ('apolloquiboloy', 'kingdomofjesuschrist'),\n",
       " (\"do'\", \"don't\"),\n",
       " ('kingdomofjesuschrist', 'bebless'),\n",
       " ('lauri', 'garrett'),\n",
       " ('logoanim', 'madeonfiverr'),\n",
       " ('mdncvc', 'ruhyuybt'),\n",
       " ('wbzpq', 'hvbj'),\n",
       " ('wltxnlcv', 'prnyztb'),\n",
       " ('acquit', 'maniac'),\n",
       " ('maryam', 'rajavi'),\n",
       " ('morninglivesabc', 'leannemana'),\n",
       " ('angi', 'motshekga'),\n",
       " ('pradhan', 'mantri'),\n",
       " ('lockdowninsa', 'lockdowninsouthafrica'),\n",
       " ('nova', 'scotia'),\n",
       " ('funk', 'akindel'),\n",
       " ('raoult', 'didier'),\n",
       " ('worldfightscorona', 'stayhomestaystrong'),\n",
       " ('adhanom', 'ghebreyesu'),\n",
       " ('babajid', 'sanwo-olu'),\n",
       " ('igf', 'logoanim'),\n",
       " ('canceleveryth', 'codvid'),\n",
       " ('followback', 'siguemeytesigo'),\n",
       " ('capolit', 'capol'),\n",
       " ('sakalnew', 'viralnew'),\n",
       " ('coronavirusourtbreak', 'thewiderwiserview'),\n",
       " ('kart', 'nach'),\n",
       " ('mantri', 'kalyan'),\n",
       " ('mikequindazzi', 'antgrasso'),\n",
       " ('nach', 'mdncvc'),\n",
       " ('powerbi', 'uipath'),\n",
       " ('ree', 'mogg'),\n",
       " ('tito', 'mboweni'),\n",
       " ('dino', 'melay'),\n",
       " ('lori', 'lightfoot'),\n",
       " ('intro', 'kmnw'),\n",
       " ('worldstar', 'wshh'),\n",
       " ('wshh', 'worldstar'),\n",
       " ('sudhakar', 'gouranga'),\n",
       " ('tasmania', 'polita'),\n",
       " ('whoemro', 'whowpro'),\n",
       " ('kkoo', 'fnpl'),\n",
       " ('koko', 'pimentel'),\n",
       " ('aytu', 'bioscienc'),\n",
       " ('thesi', 'dissert'),\n",
       " ('nigel', 'farag'),\n",
       " ('judi', 'mikovit'),\n",
       " ('premlataasopa', 'wetwokrishna'),\n",
       " ('ptioffici', 'pmln'),\n",
       " ('rudi', 'giuliani'),\n",
       " ('uddhav', 'thackeray'),\n",
       " ('greyhound', 'aokh'),\n",
       " ('covidagenparl', 'iorestoacasa'),\n",
       " ('sonu', 'sood'),\n",
       " ('comcast', 'verizon'),\n",
       " ('betsi', 'devo'),\n",
       " ('borderobserv', 'ecxcul'),\n",
       " ('economiccalendar', 'unitedkingdom'),\n",
       " ('feroci', 'rampag'),\n",
       " ('nitin', 'gadkari'),\n",
       " ('thenationnew', 'thisdayl'),\n",
       " ('torygenocid', 'wheresbori'),\n",
       " ('brett', 'crozier'),\n",
       " ('standardkenya', 'thestarkenya'),\n",
       " ('greyhound', 'wltxnlcv'),\n",
       " ('newsdesk', 'trendingnew'),\n",
       " ('tengah', 'pandemi'),\n",
       " ('vijayabaskarofl', 'drbeelaia'),\n",
       " ('yogi', 'adityanath'),\n",
       " ('financialliteraci', 'personalfin'),\n",
       " ('girlswhocod', 'womeninstem'),\n",
       " ('kyli', 'jenner'),\n",
       " ('myogioffic', 'upgovt'),\n",
       " ('pnpkakampimolabansacovid', 'rwinp'),\n",
       " ('sakaltim', 'sakalnew'),\n",
       " ('davidmalpasswbg', 'kgeorgieva'),\n",
       " ('senkamalaharri', 'capolit'),\n",
       " ('southafricalockdown', 'lockdowninsa'),\n",
       " ('horserac', 'greyhound'),\n",
       " ('nucleic', 'acid'),\n",
       " ('bastopthinkagain', 'fireandrehir'),\n",
       " ('lqmh', 'kart'),\n",
       " ('trish', 'regan'),\n",
       " ('pearl', 'harbor'),\n",
       " ('kayleigh', 'mcenani'),\n",
       " ('paulo', 'dybala'),\n",
       " ('potent', 'inhibitor'),\n",
       " ('sakal', 'sakaltim'),\n",
       " ('dayslockdownsa', 'stayhomesa'),\n",
       " ('isl', 'wight'),\n",
       " ('unitednotdivid', 'saveamerica'),\n",
       " ('babetray', 'bastopthinkagain'),\n",
       " ('antgrasso', 'fisher'),\n",
       " ('felin', 'nanni'),\n",
       " ('hipaa', 'compliant'),\n",
       " ('jyot', 'sudhakar'),\n",
       " ('obamagateg', 'pizzag'),\n",
       " ('outweigh', 'con'),\n",
       " ('whowpro', 'pahowho'),\n",
       " ('staythefhom', 'socialdistancingnow'),\n",
       " ('teampnp', 'weserveandprotect'),\n",
       " ('akwa', 'ibom'),\n",
       " ('hippocrat', 'oath'),\n",
       " ('janeruth', 'aceng'),\n",
       " ('aborigin', 'torr'),\n",
       " ('backto', 'swomen'),\n",
       " ('bodili', 'fluid'),\n",
       " ('bund', 'swedishpm'),\n",
       " ('casey', 'frey'),\n",
       " ('geonew', 'urdu'),\n",
       " ('kapilmishra', 'tajinderbagga'),\n",
       " ('mesa', 'verd'),\n",
       " ('srhr', 'srhrisessenti'),\n",
       " ('swomen', 'backto'),\n",
       " ('tcot', 'ccot'),\n",
       " ('topnew', 'globalbreak'),\n",
       " ('dailyn', 'thenationnew'),\n",
       " ('policest', 'behavioralsci'),\n",
       " ('senfeinstein', 'senkamalaharri'),\n",
       " ('newsupd', 'newsdesk'),\n",
       " ('govpritzk', 'chicagosmayor'),\n",
       " ('sakal', 'sakalnew'),\n",
       " ('vera', 'nutan'),\n",
       " ('alyssa', 'milano'),\n",
       " ('jerri', 'falwel'),\n",
       " ('asad', 'umar'),\n",
       " ('carl', 'deutsch'),\n",
       " ('puerto', 'rico'),\n",
       " ('pro', 'con'),\n",
       " ('wheat', 'flour'),\n",
       " ('rheumatoid', 'arthriti'),\n",
       " ('secazar', 'repmarkmeadow'),\n",
       " ('kita', 'bisa'),\n",
       " ('ascens', 'darktolight'),\n",
       " ('indiafightscornona', 'stayawarestaysaf'),\n",
       " ('pnpkakampimo', 'rwinp'),\n",
       " ('tengah', 'hari'),\n",
       " ('socialdistancingnow', 'highriskcovid'),\n",
       " ('thestarkenya', 'dailyn'),\n",
       " ('weserveandprotect', 'pnpkakampimo'),\n",
       " ('skinnergj', 'crnew'),\n",
       " ('coronapocalyps', 'filmyourhospit'),\n",
       " ('pkpb', 'cmco'),\n",
       " ('santa', 'clara'),\n",
       " ('bishop', 'oyedepo'),\n",
       " ('costa', 'rica'),\n",
       " ('troy', 'deeney'),\n",
       " ('hongkongprotest', 'hkpolicebrut'),\n",
       " ('hugh', 'dlaignit'),\n",
       " ('capt', 'amarind'),\n",
       " ('rishi', 'sunak'),\n",
       " ('mutahi', 'kagw'),\n",
       " ('whereisbori', 'torygenocid'),\n",
       " ('lisamurkowski', 'senatorcollin'),\n",
       " ('mede', 'foam'),\n",
       " ('jacob', 'ree'),\n",
       " ('bangkokpost', 'phuket'),\n",
       " ('egg', 'basket'),\n",
       " ('krishna', 'premlataasopa'),\n",
       " ('opioid', 'overdos'),\n",
       " ('vide', 'revert'),\n",
       " ('infrar', 'thermomet'),\n",
       " ('cautious', 'ponder'),\n",
       " ('davi', 'hanson'),\n",
       " ('amarind', 'cmopb'),\n",
       " ('checkout', 'demo'),\n",
       " ('coronarviru', 'coronaviruschina'),\n",
       " ('craig', 'spencer'),\n",
       " ('rajiv', 'bajaj'),\n",
       " ('washabl', 'reusabl'),\n",
       " ('babu', 'manu'),\n",
       " ('congresswoman', 'atroci'),\n",
       " ('dghisham', 'kkmputrajaya'),\n",
       " ('southampton', 'portsmouth'),\n",
       " ('vijayanpinarayi', 'cmokerala'),\n",
       " ('barnard', 'castl'),\n",
       " ('neet', 'neet'),\n",
       " ('sardanarohit', 'anjanaomkashyap'),\n",
       " ('wordpress', 'landingpag'),\n",
       " ('world-wid', 'world-wid')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(words_9)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-100 bigrams\n",
    "top_200_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9917\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "mwetokenizer = MWETokenizer(top_200_bigrams)\n",
    "data_tokenized_5=  dict((keys, mwetokenizer.tokenize(values)) for keys,values in data_tokenized_4.items())\n",
    "words_11 = list(chain.from_iterable(data_tokenized_5.values()))\n",
    "colloc_voc = set(words_11)\n",
    "print(len(list(colloc_voc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-22 {'janeruth_aceng', 'paulo_dybala'}\n",
      "2020-03-23 {'funk_akindel', 'tcot_ccot', 'uddhav_thackeray', 'skinnergj_crnew'}\n",
      "2020-03-24 {'adhanom_ghebreyesu', 'nirmala_sitharaman', 'sierra_leon', 'whereisbori_torygenocid', 'sardanarohit_anjanaomkashyap', 'geonew_urdu', 'staythefhom_socialdistancingnow'}\n",
      "2020-03-25 {'koko_pimentel', 'lauri_garrett', 'tasmania_polita', 'skinnergj_crnew', 'uddhav_thackeray', 'babajid_sanwo-olu', 'wltxnlcv_prnyztb', \"do'_don't\"}\n",
      "2020-03-26 {'adhanom_ghebreyesu', 'tradingeconom_macrocalendar', 'rishi_sunak', 'infrar_thermomet', 'lqmh_kart', 'janeruth_aceng', 'tejasvi_surya', 'economiccalendar_unitedkingdom', 'jerri_falwel', 'kyli_jenner', 'nach_mdncvc', 'capt_amarind'}\n",
      "2020-03-27 {'dayslockdownsa_stayhomesa', 'koko_pimentel', 'hippocrat_oath', 'skinnergj_crnew', 'covidagenparl_iorestoacasa', 'southafricalockdown_lockdowninsa', 'craig_spencer', 'hipaa_compliant'}\n",
      "2020-03-28 {'dayslockdownsa_stayhomesa', 'nigel_farag', 'rishi_sunak', 'ptioffici_pmln', 'trish_regan', 'nitin_gadkari', 'rheumatoid_arthriti', 'davi_hanson', 'skinnergj_crnew', 'capt_amarind', 'alyssa_milano', 'nucleic_acid'}\n",
      "2020-03-29 {'secazar_repmarkmeadow', 'canceleveryth_codvid', 'puerto_rico', 'yogi_adityanath', 'rudi_giuliani', 'govpritzk_chicagosmayor'}\n",
      "2020-03-30 {'tito_mboweni', 'kkoo_fnpl', 'wheat_flour', 'jerri_falwel', 'rudi_giuliani', 'carl_deutsch', 'capt_amarind', 'coronavirusourtbreak_thewiderwiserview'}\n",
      "2020-03-31 {'koko_pimentel', 'puerto_rico', 'topnew_globalbreak', 'sardanarohit_anjanaomkashyap', 'jerri_falwel', 'babajid_sanwo-olu', 'egg_basket', 'teampnp_weserveandprotect'}\n",
      "2020-04-01 {'mutahi_kagw', 'jerri_falwel', 'janeruth_aceng', 'skinnergj_crnew', 'govpritzk_chicagosmayor'}\n",
      "2020-04-02 {'pro_con', 'rheumatoid_arthriti', 'sakal_sakalnew', 'tejasvi_surya', 'kita_bisa', 'morninglivesabc_leannemana', 'pearl_harbor', 'capt_amarind'}\n",
      "2020-04-03 {'mesa_verd', 'nirmala_sitharaman', 'brett_crozier', 'vijayabaskarofl_drbeelaia', 'ptioffici_pmln', 'morninglivesabc_leannemana', 'southampton_portsmouth'}\n",
      "2020-04-04 {'aytu_bioscienc', 'jacob_ree', 'trish_regan', 'kapilmishra_tajinderbagga'}\n",
      "2020-04-05 {'nagendra_bandi', 'cautious_ponder', 'secazar_repmarkmeadow', 'horserac_greyhound', 'premlataasopa_wetwokrishna', 'mutahi_kagw', 'dino_melay', 'funk_akindel', 'asad_umar', 'rudi_giuliani', 'janeruth_aceng', 'tejasvi_surya', 'kita_bisa', 'jacob_ree', 'isl_wight', 'maryam_rajavi', 'bodili_fluid', 'wltxnlcv_prnyztb'}\n",
      "2020-04-06 {'nova_scotia', 'brett_crozier', 'funk_akindel', 'maryam_rajavi', 'jacob_ree', 'capt_amarind'}\n",
      "2020-04-07 {'adhanom_ghebreyesu', 'horserac_greyhound', 'standardkenya_thestarkenya', 'nova_scotia', 'lori_lightfoot', 'rheumatoid_arthriti', 'wltxnlcv_prnyztb', 'dailyn_thenationnew', 'teampnp_weserveandprotect'}\n",
      "2020-04-08 {'horserac_greyhound', 'rishi_sunak', 'coronarviru_coronaviruschina', 'puerto_rico', 'lori_lightfoot', 'pearl_harbor', 'wltxnlcv_prnyztb', 'alyssa_milano'}\n",
      "2020-04-09 {'koko_pimentel', 'raoult_didier', 'topnew_globalbreak', 'nutan_jyot', 'vijayanpinarayi_cmokerala', 'santa_clara', 'janeruth_aceng'}\n",
      "2020-04-10 {'adhanom_ghebreyesu', 'tito_mboweni', 'lori_lightfoot', 'costa_rica', 'whoemro_whowpro', 'pearl_harbor', 'alyssa_milano'}\n",
      "2020-04-11 {'adhanom_ghebreyesu', 'tradingeconom_macrocalendar', 'worldstar_wshh', 'jerri_falwel', 'kita_bisa', 'hippocrat_oath', 'uddhav_thackeray', 'govpritzk_chicagosmayor'}\n",
      "2020-04-12 {'foryoupag_foryou', 'puerto_rico', 'senfeinstein_senkamalaharri', 'sakal_sakalnew', 'capolit_capol', 'thesi_dissert'}\n",
      "2020-04-13 {'dayslockdownsa_stayhomesa', 'wordpress_landingpag', 'tito_mboweni', 'foryoupag_foryou', 'jyot_sudhakar', 'kkoo_fnpl', 'senfeinstein_senkamalaharri', 'vera_nutan', 'tejasvi_surya', 'carl_deutsch', 'capolit_capol', 'aborigin_torr', 'krishna_premlataasopa'}\n",
      "2020-04-14 {'horserac_greyhound', 'brett_crozier', 'worldstar_wshh', 'senfeinstein_senkamalaharri', 'rheumatoid_arthriti', 'capolit_capol', 'pradhan_mantri', 'wltxnlcv_prnyztb'}\n",
      "2020-04-15 {'dino_melay', 'nitin_gadkari', 'senfeinstein_senkamalaharri', 'policest_behavioralsci', 'capolit_capol', 'coronavirusourtbreak_thewiderwiserview', 'babu_manu'}\n",
      "2020-04-16 {'nagendra_bandi', 'horserac_greyhound', 'jyot_sudhakar', 'kkoo_fnpl', 'vera_nutan', 'senfeinstein_senkamalaharri', 'akwa_ibom', 'carl_deutsch', 'capolit_capol', 'krishna_premlataasopa'}\n",
      "2020-04-17 {'horserac_greyhound', 'whoemro_whowpro', 'nitin_gadkari', 'wltxnlcv_prnyztb', 'unitednotdivid_saveamerica'}\n",
      "2020-04-18 {'nagendra_bandi', 'horserac_greyhound', 'dino_melay', 'funk_akindel', 'hongkongprotest_hkpolicebrut', 'world-wid_world-wid'}\n",
      "2020-04-19 {'cautious_ponder', 'opioid_overdos', 'secazar_repmarkmeadow', 'tradingeconom_macrocalendar', 'horserac_greyhound', 'standardkenya_thestarkenya', 'puerto_rico', 'funk_akindel', 'feroci_rampag', 'betsi_devo', 'economiccalendar_unitedkingdom', 'kayleigh_mcenani', 'wltxnlcv_prnyztb', 'dailyn_thenationnew'}\n",
      "2020-04-20 {'horserac_greyhound', 'mutahi_kagw', 'nova_scotia', 'maryam_rajavi', 'lqmh_kart', 'yogi_adityanath', 'akwa_ibom', 'srhr_srhrisessenti', 'nach_mdncvc', 'wltxnlcv_prnyztb'}\n",
      "2020-04-21 {'coronarviru_coronaviruschina', 'dino_melay', 'vijayabaskarofl_drbeelaia', 'feroci_rampag', 'roboinfo_robotex', 'babajid_sanwo-olu'}\n",
      "2020-04-22 {'nagendra_bandi', 'horserac_greyhound', 'sudhakar_gouranga', 'yogi_adityanath', 'vera_nutan', 'santa_clara', 'alyssa_milano', 'krishna_premlataasopa', 'casey_frey'}\n",
      "2020-04-23 {'horserac_greyhound', 'davidmalpasswbg_kgeorgieva', 'lisamurkowski_senatorcollin', 'santa_clara', 'agenparl_covidagenparl', 'uddhav_thackeray', 'pearl_harbor', 'newsupd_newsdesk'}\n",
      "2020-04-24 {'akwa_ibom', 'hippocrat_oath', 'babajid_sanwo-olu', 'mdncvc_ruhyuybt', 'southafricalockdown_lockdowninsa'}\n",
      "2020-04-25 {'infrar_thermomet', 'vijayabaskarofl_drbeelaia', 'rudi_giuliani', 'policest_behavioralsci', 'logoanim_madeonfiverr'}\n",
      "2020-04-26 {'socialsel_digitalsel', 'hugh_dlaignit', 'lauri_garrett', 'lori_lightfoot', 'whereisbori_torygenocid', 'geonew_urdu', 'teampnp_weserveandprotect', 'staythefhom_socialdistancingnow'}\n",
      "2020-04-27 {'mutahi_kagw', 'nigel_farag', 'canceleveryth_codvid', 'puerto_rico', 'davidmalpasswbg_kgeorgieva', 'meaningfulgrowth_geniouxmg', 'nitin_gadkari', 'akwa_ibom', 'powerbi_uipath', 'policest_behavioralsci', 'teampnp_weserveandprotect', 'judi_mikovit'}\n",
      "2020-04-28 {'topnew_globalbreak', 'maryam_rajavi', 'tejasvi_surya', 'bund_swedishpm', 'sakalnew_viralnew', 'sakal_sakaltim'}\n",
      "2020-04-29 {'tcot_ccot', 'nigel_farag', 'mikequindazzi_antgrasso', 'meaningfulgrowth_geniouxmg', 'washabl_reusabl', 'akwa_ibom', 'roboinfo_robotex', 'uddhav_thackeray', 'paulo_dybala', 'unitednotdivid_saveamerica'}\n",
      "2020-04-30 {'vide_revert', 'nigel_farag', 'coronarviru_coronaviruschina', 'angi_motshekga', 'sakal_sakaltim', 'akwa_ibom', 'sakal_sakalnew', 'bund_swedishpm', 'sakalnew_viralnew', 'pradhan_mantri', 'capt_amarind', 'followback_siguemeytesigo'}\n",
      "2020-05-01 {'sperm_smoothi', 'apolloquiboloy_kingdomofjesuschrist', 'canceleveryth_codvid', 'krishna_premlataasopa', 'angi_motshekga', 'puerto_rico', 'nutan_jyot', 'asad_umar', 'whoemro_whowpro', 'santa_clara', 'akwa_ibom', 'hippocrat_oath', 'morninglivesabc_leannemana', 'comcast_verizon', 'coronapocalyps_filmyourhospit', 'followback_siguemeytesigo', 'unitednotdivid_saveamerica'}\n",
      "2020-05-02 {'sperm_smoothi', 'wordpress_landingpag', 'foryoupag_foryou', 'wbzpq_hvbj', 'agenparl_covidagenparl', 'roboinfo_robotex', 'alyssa_milano', 'aytu_bioscienc'}\n",
      "2020-05-03 {'asitha_jayawardena', 'rajiv_bajaj', 'infrar_thermomet', 'outweigh_con', 'neet_neet', 'nitin_gadkari', 'betsi_devo', 'igf_logoanim', 'intro_kmnw', 'unitednotdivid_saveamerica'}\n",
      "2020-05-04 {'asad_umar', 'wheat_flour', 'maryam_rajavi', 'southampton_portsmouth', 'dlaignit_socialsel'}\n",
      "2020-05-05 {'nigel_farag', 'sierra_leon', 'dino_melay', 'outweigh_con', 'policest_behavioralsci', 'igf_logoanim', 'intro_kmnw', 'judi_mikovit', 'paulo_dybala'}\n",
      "2020-05-06 {'cautious_ponder', 'wordpress_landingpag', 'apolloquiboloy_kingdomofjesuschrist', 'tito_mboweni', 'rishi_sunak', 'wbzpq_hvbj', 'tengah_pandemi', 'janeruth_aceng', 'morninglivesabc_leannemana', 'babajid_sanwo-olu', \"do'_don't\", 'followback_siguemeytesigo'}\n",
      "2020-05-07 {'nagendra_bandi', 'puerto_rico', 'lisamurkowski_senatorcollin', 'lauri_garrett', 'bangkokpost_phuket', 'davi_hanson', 'borderobserv_ecxcul', 'pearl_harbor', 'pradhan_mantri', 'paulo_dybala', 'followback_siguemeytesigo', 'dlaignit_socialsel'}\n",
      "2020-05-08 {'wordpress_landingpag', 'apolloquiboloy_kingdomofjesuschrist', 'mede_foam', 'kkoo_fnpl', 'davi_hanson', 'janeruth_aceng', 'borderobserv_ecxcul', 'roboinfo_robotex', 'carl_deutsch', 'isl_wight', 'judi_mikovit'}\n",
      "2020-05-09 {'backto_swomen', 'apolloquiboloy_kingdomofjesuschrist', 'tradingeconom_macrocalendar', 'infrar_thermomet', 'wbzpq_hvbj', 'costa_rica', 'worldstar_wshh', 'kkoo_fnpl', 'akwa_ibom', 'carl_deutsch', 'isl_wight'}\n",
      "2020-05-10 {'koko_pimentel', 'nigel_farag', 'wbzpq_hvbj', 'asad_umar', 'washabl_reusabl', 'nitin_gadkari', 'sakal_sakalnew', 'swomen_backto', 'covidagenparl_iorestoacasa', 'myogioffic_upgovt', 'followback_siguemeytesigo'}\n",
      "2020-05-11 {'bishop_oyedepo', 'socialsel_digitalsel', 'wordpress_landingpag', 'rishi_sunak', 'hugh_dlaignit', 'borderobserv_ecxcul', 'isl_wight', 'capt_amarind', 'coronapocalyps_filmyourhospit'}\n",
      "2020-05-12 {'mede_foam', 'rishi_sunak', 'isl_wight', 'potent_inhibitor', 'hipaa_compliant'}\n",
      "2020-05-13 {'nirmala_sitharaman', 'mikequindazzi_antgrasso', 'srhr_srhrisessenti', 'asad_umar', 'nitin_gadkari', 'santa_clara', 'akwa_ibom', 'judi_mikovit', 'potent_inhibitor'}\n",
      "2020-05-14 {'adhanom_ghebreyesu', 'apolloquiboloy_kingdomofjesuschrist', 'checkout_demo', 'tito_mboweni', 'puerto_rico', 'tengah_hari', 'tasmania_polita', 'costa_rica', 'janeruth_aceng', 'felin_nanni', 'teampnp_weserveandprotect', 'pnpkakampimolabansacovid_rwinp'}\n",
      "2020-05-15 {'bishop_oyedepo', 'mede_foam', 'nirmala_sitharaman', 'raoult_didier', 'davidmalpasswbg_kgeorgieva', 'bangkokpost_phuket', 'congresswoman_atroci', 'lqmh_kart', 'powerbi_uipath', 'tengah_pandemi', 'nach_mdncvc', 'teampnp_weserveandprotect', 'judi_mikovit', 'nucleic_acid'}\n",
      "2020-05-16 {'bishop_oyedepo', 'checkout_demo', 'indiafightscornona_stayawarestaysaf', 'mutahi_kagw', 'nirmala_sitharaman', 'foryoupag_foryou', 'davidmalpasswbg_kgeorgieva', 'meaningfulgrowth_geniouxmg', 'betsi_devo', 'girlswhocod_womeninstem', 'janeruth_aceng', 'agenparl_covidagenparl', 'akwa_ibom', 'financialliteraci_personalfin', 'logoanim_madeonfiverr'}\n",
      "2020-05-17 {'asitha_jayawardena', 'mede_foam', 'nirmala_sitharaman', 'sierra_leon', 'meaningfulgrowth_geniouxmg', 'lqmh_kart', 'nach_mdncvc', 'hongkongprotest_hkpolicebrut', 'coronavirusourtbreak_thewiderwiserview'}\n",
      "2020-05-18 {'asitha_jayawardena', 'meaningfulgrowth_geniouxmg', 'akwa_ibom', 'powerbi_uipath', 'pearl_harbor', 'isl_wight', 'igf_logoanim', 'intro_kmnw', 'newsupd_newsdesk', 'coronavirusourtbreak_thewiderwiserview', 'nucleic_acid'}\n",
      "2020-05-19 {'bishop_oyedepo', 'tradingeconom_macrocalendar', 'indiafightscornona_stayawarestaysaf', 'rishi_sunak', 'lauri_garrett', 'betsi_devo', 'economiccalendar_unitedkingdom'}\n",
      "2020-05-20 {'troy_deeney', 'pkpb_cmco', 'rishi_sunak', 'costa_rica', 'trish_regan', 'kayleigh_mcenani', 'igf_logoanim', 'intro_kmnw', 'felin_nanni', 'pnpkakampimolabansacovid_rwinp', 'thesi_dissert'}\n",
      "2020-05-21 {'troy_deeney', 'canceleveryth_codvid', 'sierra_leon', 'tengah_hari', 'costa_rica', 'acquit_maniac', 'agenparl_covidagenparl', 'thesi_dissert', 'kita_bisa', 'sakalnew_viralnew', 'sakal_sakaltim', 'alyssa_milano', \"do'_don't\", 'nucleic_acid'}\n",
      "2020-05-22 {'opioid_overdos', 'worldfightscorona_stayhomestaystrong', 'nova_scotia', 'wbzpq_hvbj', 'puerto_rico', 'betsi_devo', 'janeruth_aceng', 'uddhav_thackeray', 'alyssa_milano', 'southafricalockdown_lockdowninsa', 'unitednotdivid_saveamerica'}\n",
      "2020-05-23 {'apolloquiboloy_kingdomofjesuschrist', 'nirmala_sitharaman', 'washabl_reusabl', 'wheat_flour', 'hippocrat_oath', 'isl_wight', 'kayleigh_mcenani', 'southafricalockdown_lockdowninsa', 'teampnp_weserveandprotect', \"do'_don't\", 'pnpkakampimo_rwinp'}\n",
      "2020-05-24 {'sperm_smoothi', 'tcot_ccot', 'mutahi_kagw', 'sierra_leon', 'mikequindazzi_antgrasso', 'nitin_gadkari', 'tejasvi_surya', 'jacob_ree', 'alyssa_milano', 'newsupd_newsdesk', 'babetray_bastopthinkagain', 'southafricalockdown_lockdowninsa'}\n",
      "2020-05-25 {'sperm_smoothi', 'sonu_sood', 'feroci_rampag', 'barnard_castl', 'borderobserv_ecxcul', 'capt_amarind', 'babetray_bastopthinkagain', 'craig_spencer'}\n",
      "2020-05-26 {'artcal_artistcal', 'pro_con', 'apolloquiboloy_kingdomofjesuschrist', 'sonu_sood', 'pkpb_cmco', 'dino_melay', 'artistscal_artpromot', 'barnard_castl', 'newsupd_newsdesk', 'southafricalockdown_lockdowninsa', 'potent_inhibitor', 'craig_spencer'}\n",
      "2020-05-27 {'nagendra_bandi', 'staythefhom_socialdistancingnow', 'lisamurkowski_senatorcollin', 'lori_lightfoot', 'whereisbori_torygenocid', 'worldstar_wshh', 'barnard_castl', 'vijayanpinarayi_cmokerala', 'nitin_gadkari', 'sakal_sakalnew', 'janeruth_aceng', 'comcast_verizon', 'kayleigh_mcenani', 'teampnp_weserveandprotect', 'potent_inhibitor', 'pnpkakampimo_rwinp'}\n",
      "2020-05-28 {'artcal_artistcal', 'socialsel_digitalsel', 'pro_con', 'sonu_sood', 'nigel_farag', 'hugh_dlaignit', 'troy_deeney', 'tcot_ccot', 'mikequindazzi_antgrasso', 'lauri_garrett', 'babetray_bastopthinkagain', 'artistscal_artpromot', 'worldfightscorona_stayhomestaystrong', 'tasmania_polita', 'betsi_devo', 'jerri_falwel', 'intro_kmnw', \"do'_don't\"}\n",
      "2020-05-29 {'sonu_sood', 'rishi_sunak', 'worldfightscorona_stayhomestaystrong', 'akwa_ibom', 'acquit_maniac', 'kayleigh_mcenani', 'unitednotdivid_saveamerica'}\n",
      "2020-05-30 {'artcal_artistcal', 'sonu_sood', 'nigel_farag', 'rishi_sunak', 'sierra_leon', 'puerto_rico', 'artistscal_artpromot', 'barnard_castl', 'capt_amarind', 'alyssa_milano', 'southafricalockdown_lockdowninsa'}\n",
      "2020-05-31 {'ascens_darktolight', 'worldfightscorona_stayhomestaystrong', 'raoult_didier', 'brett_crozier', 'hongkongprotest_hkpolicebrut'}\n",
      "2020-06-01 {'ascens_darktolight', 'standardkenya_thestarkenya', 'davidmalpasswbg_kgeorgieva', 'angi_motshekga', 'jerri_falwel', 'comcast_verizon', 'capt_amarind', 'dailyn_thenationnew', 'alyssa_milano', 'aytu_bioscienc'}\n",
      "2020-06-02 {'nagendra_bandi', 'infrar_thermomet', 'rishi_sunak', 'neet_neet', 'kapilmishra_tajinderbagga', 'kyli_jenner'}\n",
      "2020-06-03 {'troy_deeney', 'asad_umar', 'washabl_reusabl', 'kkoo_fnpl', 'akwa_ibom', 'borderobserv_ecxcul', 'barnard_castl', 'roboinfo_robotex', 'jacob_ree', 'carl_deutsch', 'egg_basket', 'mdncvc_ruhyuybt', 'teampnp_weserveandprotect', 'babu_manu', 'nucleic_acid'}\n",
      "2020-06-04 {'artcal_artistcal', 'pro_con', 'weserveandprotect_pnpkakampimo', 'rajiv_bajaj', 'babetray_bastopthinkagain', 'artistscal_artpromot', 'barnard_castl', 'kkoo_fnpl', 'jacob_ree', 'roboinfo_robotex', 'comcast_verizon', 'carl_deutsch', 'mdncvc_ruhyuybt'}\n",
      "2020-06-05 {'mutahi_kagw', 'canceleveryth_codvid', 'sierra_leon', 'aborigin_torr', 'davidmalpasswbg_kgeorgieva', 'costa_rica', 'asad_umar', 'kkoo_fnpl', 'acquit_maniac', 'tengah_pandemi', 'carl_deutsch', \"do'_don't\", 'capt_amarind', 'myogioffic_upgovt', 'financialliteraci_personalfin', 'coronavirusourtbreak_thewiderwiserview'}\n",
      "2020-06-06 {'backto_swomen', 'nova_scotia', 'whereisbori_torygenocid', 'jacob_ree', 'staythefhom_socialdistancingnow'}\n",
      "2020-06-07 {'asitha_jayawardena', 'artcal_artistcal', 'sonu_sood', 'puerto_rico', 'raoult_didier', 'artistscal_artpromot', 'girlswhocod_womeninstem', 'borderobserv_ecxcul', 'jacob_ree', 'sakalnew_viralnew', 'bodili_fluid', 'sakal_sakaltim', 'obamagateg_pizzag', 'pradhan_mantri', 'teampnp_weserveandprotect', 'coronavirusourtbreak_thewiderwiserview', 'newsupd_newsdesk'}\n",
      "2020-06-08 {'artcal_artistcal', 'vide_revert', 'angi_motshekga', 'lauri_garrett', 'artistscal_artpromot', 'agenparl_covidagenparl', 'janeruth_aceng', 'dghisham_kkmputrajaya', 'obamagateg_pizzag', 'southafricalockdown_lockdowninsa', 'craig_spencer'}\n",
      "2020-06-09 {'infrar_thermomet', 'mesa_verd', 'puerto_rico', 'dghisham_kkmputrajaya', 'mdncvc_ruhyuybt', 'aytu_bioscienc', 'coronavirusourtbreak_thewiderwiserview', 'paulo_dybala'}\n",
      "2020-06-10 {'asitha_jayawardena', 'artcal_artistcal', 'puerto_rico', 'meaningfulgrowth_geniouxmg', 'craig_spencer', 'artistscal_artpromot', 'kkoo_fnpl', 'carl_deutsch', 'babetray_bastopthinkagain', 'unitednotdivid_saveamerica', 'thesi_dissert'}\n"
     ]
    }
   ],
   "source": [
    "for key in data_tokenized_4.keys():\n",
    "    diff = set(data_tokenized_5[key])-set(data_tokenized_4[key])\n",
    "    if len(diff) != 0:\n",
    "        print (key, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets = []\n",
    "bag_of_words = []\n",
    "for sheet, tokens in data_tokenized_5.items():\n",
    "    sheets.append(sheet)\n",
    "    txt = ' '.join(tokens)\n",
    "    bag_of_words.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(input = 'content',analyzer = \"word\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 9794)\n"
     ]
    }
   ],
   "source": [
    "data_features = vectorizer.fit_transform(bag_of_words)\n",
    "print (data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9794"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_vocab = vectorizer.get_feature_names()\n",
    "len(list(full_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Write Vocab to fie and creating a Dict for VOCAB\n",
    "full_vocab=set(full_vocab)\n",
    "vocab_dict = {}\n",
    "full_vocab=sorted(full_vocab)\n",
    "save_file = open(\"arushi_Vocab.txt\", 'w')\n",
    "for count, item in enumerate(full_vocab):\n",
    "    #print(item, count)\n",
    "    vocab_dict[item] = item\n",
    "\n",
    "    save_file.write(\"%s:%s\\n\"%(item,count))\n",
    "  \n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = open(\"arushi_countvec.txt\", 'w')\n",
    "vocab3=vectorizer.get_feature_names()\n",
    "cx = data_features.tocoo() # return the coordinate representation of a sparse matrix\n",
    "counter=0\n",
    "save_file.write(sheets[counter])\n",
    "for i,j,v in itertools.zip_longest(cx.row, cx.col, cx.data):\n",
    "    if i==counter:\n",
    "        save_file.write(',' + str(j) + ':' + str(v))\n",
    "    else:\n",
    "        counter+=1\n",
    "        save_file.write('\\n'+ sheets[counter])\n",
    "    \n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
